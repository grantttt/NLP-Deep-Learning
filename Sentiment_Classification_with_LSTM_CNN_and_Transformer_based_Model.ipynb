{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLNg_Puse6EX"
   },
   "source": [
    "# Text Classification for the IMDB Dataset using DL\n",
    "**Objective:** classify the IMDB Reviews into positive or negative. <br>\n",
    "In this notebook we explore different DL-based text classification models and compare their performance. <br>\n",
    "The notebook is coded with Keras and explores the following three architectures:\n",
    "1. CNN-based models with and without pre-trained embeddings\n",
    "2. LSTM-based models with and without pre-trained embeddings\n",
    "3. Transformer-based models with and without pre-trained embeddings (for you to do)\n",
    "This notebook needs a GPU; google colab could be used.\n",
    "**Useful documentation** <br>\n",
    "- [Pre-trained embeddings with Keras](https://keras.io/examples/nlp/pretrained_word_embeddings/) \n",
    "- [Sentiment classification with LSTM keras](https://slundberg.github.io/shap/notebooks/deep_explainer/Keras%20LSTM%20for%20IMDB%20Sentiment%20Classification.html) \n",
    "# Installation of needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eOJLveJqtEO3",
    "outputId": "d7bb877d-7ea9-4dad-a54c-fe542ba2c716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.32.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.22.4)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.53.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (16.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.6.1)\n",
      "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.7)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (0.0.4)\n",
      "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.1.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "#!pip install numpy==1.19.5\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUL_X6mjrvSS"
   },
   "source": [
    "# Importing needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xqUcb7NBb5--"
   },
   "outputs": [],
   "source": [
    "import os, sys, numpy as np, pandas as pd\n",
    "from zipfile import ZipFile\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Qh2mdvdaJqP",
    "outputId": "ebf65979-99b0-42f1-e91c-0d6f5ef91ed1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  4 18:07:01 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   36C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJAHdXuVaLYJ",
    "outputId": "ad830e77-1dfa-4c14-c67d-f45f082270e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime has 27.3 gigabytes of available RAM\n",
      "\n",
      "You are using a high-RAM runtime!\n"
     ]
    }
   ],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZLq2c-1r0kj"
   },
   "source": [
    "# Downloading dataset & pre-trained GLOVE embeddings\n",
    "1. [GLOVE](http://nlp.stanford.edu/data/glove.6B.zip)\n",
    "2. [IMDB dataset](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz)\n",
    "\n",
    "They are both zipped, thus we need to unzip them. <br>\n",
    "We will put the data and pre-trained mebddings into a folder called Data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iiMzjo68pFNO",
    "outputId": "fb860e39-0fbe-4000-fc39-351c0fc74e49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nnawrNPbrNib"
   },
   "outputs": [],
   "source": [
    "GLOVE_DIR = '/content/drive/MyDrive/glove'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFOrOK8Ely_A"
   },
   "outputs": [],
   "source": [
    "#if not os.path.exists('Data'):\n",
    "#    os.mkdir('Data')\n",
    "#if not os.path.exists('Data/glove.6B') : \n",
    "#    temp='glove.6B.zip' \n",
    "#    file = ZipFile(temp)  \n",
    "#    file.extractall('Data/glove.6B') \n",
    "#    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qvl1qb78fUib"
   },
   "outputs": [],
   "source": [
    "#BASE_DIR = 'Data'\n",
    "#GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "\n",
    "df_train = pd.read_excel('/content/drive/MyDrive/train_set_imdb_reviews.xlsx')\n",
    "df_test = pd.read_excel('/content/drive/MyDrive/test_set_imdb_reviews.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZeD2tYSoQqZ"
   },
   "source": [
    "# EDA\n",
    "- Explore both datasets\n",
    "- Clean the datasets: !!! The cleaning steps should be deduced following the exploration done on the train set not on the test set to garantee **no data leakage**. However, it is applied on both. \n",
    "- Check if the dataset is balanced or not \n",
    "- Bonus: fix the imbalance if it turns out to be the case\n",
    "\n",
    "At the end of the EDA, set the cleaned reviews (texts) to the variables ``train_texts`` and ``test_texts`` and the sentiments to ``train_labels`` and ``test_labels``. <br>\n",
    "If you failed this step, use the following commands: <br>\n",
    "1. ``train_texts = df_train.reviews.apply(lambda x: str(x)).tolist()``\n",
    "2. ``test_texts = df_train.reviews.apply(lambda x: str(x)).tolist()``\n",
    "3. ``train_labels = df_train.sentiment.tolist()``\n",
    "4. ``test_labels = df_test.sentiment.tolist()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "Q2OEnQQKtCN0",
    "outputId": "c9d5a269-a951-4265-b098-d9706c56f87d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-76e8604e-e911-4fb0-a846-24581681a820\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Story of a man who has unnatural feelings for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Airport '77 starts as a brand new luxury 747 p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This film lacked something I couldn't put my f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sorry everyone,,, I know this is supposed to b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When I was little my parents took me along to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>Seeing as the vote average was pretty low, and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>The plot had some wretched, unbelievable twist...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>I am amazed at how this movie(and most others ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>A Christmas Together actually came before my t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>Working-class romantic drama from director Mar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-76e8604e-e911-4fb0-a846-24581681a820')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-76e8604e-e911-4fb0-a846-24581681a820 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-76e8604e-e911-4fb0-a846-24581681a820');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                 reviews  sentiment\n",
       "0      Story of a man who has unnatural feelings for ...          0\n",
       "1      Airport '77 starts as a brand new luxury 747 p...          0\n",
       "2      This film lacked something I couldn't put my f...          0\n",
       "3      Sorry everyone,,, I know this is supposed to b...          0\n",
       "4      When I was little my parents took me along to ...          0\n",
       "...                                                  ...        ...\n",
       "24995  Seeing as the vote average was pretty low, and...          1\n",
       "24996  The plot had some wretched, unbelievable twist...          1\n",
       "24997  I am amazed at how this movie(and most others ...          1\n",
       "24998  A Christmas Together actually came before my t...          1\n",
       "24999  Working-class romantic drama from director Mar...          1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "O1KZRvZ3tHcs",
    "outputId": "a08dbb52-bab7-41e4-b408-5241c1f13546"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-213d9958-82e3-458b-a832-498b00b2e808\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>I was extraordinarily impressed by this film. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>Although I'm not a golf fan, I attended a snea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>From the start of \"The Edge Of Love\", the view...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>This movie, with all its complexity and subtle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>I've seen this story before but my kids haven'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-213d9958-82e3-458b-a832-498b00b2e808')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-213d9958-82e3-458b-a832-498b00b2e808 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-213d9958-82e3-458b-a832-498b00b2e808');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                 reviews  sentiment\n",
       "0      Once again Mr. Costner has dragged out a movie...          0\n",
       "1      This is an example of why the majority of acti...          0\n",
       "2      First of all I hate those moronic rappers, who...          0\n",
       "3      Not even the Beatles could write songs everyon...          0\n",
       "4      Brass pictures (movies is not a fitting word f...          0\n",
       "...                                                  ...        ...\n",
       "24995  I was extraordinarily impressed by this film. ...          1\n",
       "24996  Although I'm not a golf fan, I attended a snea...          1\n",
       "24997  From the start of \"The Edge Of Love\", the view...          1\n",
       "24998  This movie, with all its complexity and subtle...          1\n",
       "24999  I've seen this story before but my kids haven'...          1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UM6YuS_FxqDV",
    "outputId": "5f025b3a-8db7-4362-c580-a43c9e726b6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12500\n",
       "1    12500\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36ePKkaExuy0",
    "outputId": "617c1b9f-5e8d-4ce1-a98a-768ade52bfb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12500\n",
       "1    12500\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "wPDYE6pgtbxx",
    "outputId": "f81e6e68-c303-40ae-a2bc-cd60a1e27d21"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HO-677-KyqkK",
    "outputId": "1db6f8a5-ce8d-4d1f-c17c-c60f441d8b5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviews      object\n",
       "sentiment     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5B7B9r_Tzlz0",
    "outputId": "8e288590-901c-46b9-c317-b7eef055a476"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3504    0\n",
       "Name: reviews, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_integer = df_train['reviews'].apply(lambda x: isinstance(x, int))\n",
    "df_train.loc[is_integer, 'reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xr2w1IL71bX3",
    "outputId": "f69ddef1-be13-4245-dd82-fccf73bcc905"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2950    0\n",
       "Name: reviews, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_integer = df_test['reviews'].apply(lambda x: isinstance(x, int))\n",
    "df_test.loc[is_integer, 'reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HJsQM_uT0WpO"
   },
   "outputs": [],
   "source": [
    "df_train.drop([3504],inplace=True)\n",
    "df_test.drop([2950],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GzK0tSy0oQqZ",
    "outputId": "faadd271-b9cf-4ff6-be3d-7d6b8f78e7fa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "def preprocess_corpus(texts):\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    texts = [' '.join([word for word in text.split() if word.lower() not in stop_words]) for text in texts]\n",
    "\n",
    "    texts = [''.join([char for char in text if char not in string.punctuation]) for text in texts]\n",
    "\n",
    "    texts = [''.join([char for char in text if not char.isdigit()]) for text in texts]\n",
    "\n",
    "    texts = [text.lower() for text in texts]\n",
    "    \n",
    "    return texts\n",
    "\n",
    "train_texts = preprocess_corpus(df_train.reviews.tolist())\n",
    "test_texts = preprocess_corpus(df_test.reviews.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYTsmAShoQqa"
   },
   "outputs": [],
   "source": [
    "### uncomment the following if you fail the cleaning\n",
    "#train_texts = df_train.reviews.apply(lambda x: str(x)).tolist()\n",
    "# test_texts = df_test.reviews.apply(lambda x: str(x)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "NdfB-w5XoQqa"
   },
   "outputs": [],
   "source": [
    "train_labels = df_train.sentiment.tolist()\n",
    "test_labels = df_test.sentiment.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmifkoA8b5_N"
   },
   "source": [
    "# Tokenization of sentences using keras Tokenizer\n",
    "\n",
    "In keras, unlike pytorch, the Tokenizer not only splits the sentence into words but also convert words into their ids.<br>\n",
    "AS we have mentioned in class, keras is a high level layer on top of tensoflow implemented to allow novice DL users (more precisely traditional ML users) to develop DL models. <br>\n",
    "\n",
    "**Remember**, the pre-processing is learnt by looking at the train dataset only to garantee **no data leakage**, and it is applied on both datasets. \n",
    "&rarr; we fit the tokenizer on training data, then use it to tokenize both datasets. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QhhqM0Jdd7fs",
    "outputId": "bf940ccb-d25d-43ab-f264-74d6250bb38c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 118411 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "#Vectorize these text samples into a 2D integer tensor using Keras Tokenizer \n",
    "# \n",
    "MAX_NUM_WORDS = 20000 \n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS) \n",
    "tokenizer.fit_on_texts(train_texts) \n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts) #Converting text to a vector of word indexes \n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts) \n",
    "word_index = tokenizer.word_index \n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SDDSfEjVWdp2",
    "outputId": "1d90d5f1-5ab9-401d-a67e-414aea396a14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13,\n",
       " 52,\n",
       " 7470,\n",
       " 1283,\n",
       " 4534,\n",
       " 405,\n",
       " 527,\n",
       " 53,\n",
       " 1191,\n",
       " 363,\n",
       " 1673,\n",
       " 126,\n",
       " 10654,\n",
       " 7589,\n",
       " 213,\n",
       " 591,\n",
       " 2009,\n",
       " 1011,\n",
       " 2932,\n",
       " 834,\n",
       " 5302,\n",
       " 368,\n",
       " 2514,\n",
       " 1673,\n",
       " 125,\n",
       " 10,\n",
       " 716,\n",
       " 1210,\n",
       " 741,\n",
       " 152,\n",
       " 1386,\n",
       " 8,\n",
       " 986,\n",
       " 591,\n",
       " 554,\n",
       " 13882,\n",
       " 316,\n",
       " 9,\n",
       " 25,\n",
       " 2161,\n",
       " 194,\n",
       " 683,\n",
       " 755,\n",
       " 13883,\n",
       " 1591,\n",
       " 594,\n",
       " 47,\n",
       " 133,\n",
       " 30,\n",
       " 6,\n",
       " 522,\n",
       " 621,\n",
       " 21,\n",
       " 621,\n",
       " 300,\n",
       " 3387,\n",
       " 12945,\n",
       " 16957,\n",
       " 8233,\n",
       " 34,\n",
       " 3228]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnIeFte7oQqc"
   },
   "source": [
    "Since we are dealing with a classical ML/DL model, input dimension should always be fixed. <br>\n",
    "As in a traditional ML model, the number of attributes/features/columns should be fixed, in a DL model, the input dimension should be fixed as well. <br>\n",
    "In our case, the input features are sentences i.e. list of words. In order to make sure that the input has a fixed size, i.e. the sentences having the same size, we will need to fix a max length (MAX_LEN) parameter, which is the maximum number of words composing a sentence. <br>\n",
    "You might ask yourselves, But every sentence has a different set of words, shouldn't we create an input size that is equal to the number of unique words in our corpus? <br>\n",
    "The answer is No, because, we will never deal with words, we will deal with embeddings such that all words are embedded with vectors having the same dimension $d$ &rarr; every sentence of our corpus will be transformed into an input of size MAX_LEN $\\times d$ &rarr; our input will have the same size. <br>\n",
    "Thus: <br>\n",
    "- sentences with number of words > than MAX_LEN will be truncated; we chose a post truncating i.e., the first MAX_LEN are retained and the remaining words are removed. \n",
    "- sentences with number of words < than MAX_LEN will be padded; we chose a post padding i.e., the 0 id will be added after the ids of the words present in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_e0V1-bBb5_d"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 1000\n",
    "trainvalid_data = pad_sequences(sequences=train_sequences, maxlen=MAX_LEN, padding='post', truncating='post', value=0.0)\n",
    "test_data = pad_sequences(sequences=test_sequences, maxlen=MAX_LEN, padding='post', truncating='post', value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tmCKUGcMoQqd",
    "outputId": "39fa4fc8-c596-4b10-9501-59a6e395936f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   13,    52,  7470,  1283,  4534,   405,   527,    53,  1191,\n",
       "          363,  1673,   126, 10654,  7589,   213,   591,  2009,  1011,\n",
       "         2932,   834,  5302,   368,  2514,  1673,   125,    10,   716,\n",
       "         1210,   741,   152,  1386,     8,   986,   591,   554, 13882,\n",
       "          316,     9,    25,  2161,   194,   683,   755, 13883,  1591,\n",
       "          594,    47,   133,    30,     6,   522,   621,    21,   621,\n",
       "          300,  3387, 12945, 16957,  8233,    34,  3228,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0], dtype=int32),\n",
       " array([  347, 12017,  3179,     2,   140,  1095,  1585,  1088,  1191,\n",
       "         1965,  2144,   749,   360,    29,    79,  2496,  4349,    35,\n",
       "         1587,   304,   229,  1497,    15,   206,    10,   360,    35,\n",
       "           11,   360,  9324, 13296, 16719,   337,   162,   456,  1159,\n",
       "          135,    47,   158,   237,    93,   170,  3895,  4349, 12002,\n",
       "          630,  1715, 12017,   307,    16,   400,   265,    28,   118,\n",
       "         4386, 12017,   599,    79,  2496,   475, 16719,  2583,    40,\n",
       "         2476, 16436, 14051,  1124,   193,    27,   292,  1487,   460,\n",
       "          159,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0], dtype=int32))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainvalid_data[0], test_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkb7HRyBoQqd"
   },
   "source": [
    "# Converting the target into a categorical tensor variable for DL model training\n",
    "Keras implements the command ``to_categorical``, it transforms each label into a one-hot encode array of dimension = unique number of categories and sets the value 1 on the index i if the data sample belongs to the category i else 0. With to categorical, if an input belongs to several categories at a time, the label would contain several 1. <br>\n",
    "Here there is 2 catgories: neg and pos &rarr; the dimension is 2. <br>\n",
    "Example: the target of a review with a pos review is converted with ``to_categorical`` to an ``array([0,1])``, while the target of a review with a neg review is converted with ``to_categorical`` to an ``array([1,0])``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "--6ePqdkoQqe"
   },
   "outputs": [],
   "source": [
    "trainvalid_labels = to_categorical(np.asarray(train_labels))\n",
    "test_labels = to_categorical(np.asarray(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kn4kOrUGoQqe",
    "outputId": "2919f73c-8b8a-4a89-a617-ece2494a2abd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, array([0., 1.], dtype=float32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[12500] , trainvalid_labels[12500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcMuP6buoQqf"
   },
   "source": [
    "# Split the training data into a training set and a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "4efyzlu7oQqf"
   },
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "indices = np.arange(trainvalid_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "trainvalid_data = trainvalid_data[indices]\n",
    "trainvalid_labels = trainvalid_labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * trainvalid_data.shape[0])\n",
    "x_train = trainvalid_data[:-num_validation_samples]\n",
    "y_train = trainvalid_labels[:-num_validation_samples]\n",
    "x_val = trainvalid_data[-num_validation_samples:]\n",
    "y_val = trainvalid_labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q93rZUKsoQqf"
   },
   "source": [
    "# Convert the token ids into embedding vectors\n",
    "1. Extract embeddings from glove.6B.100d.txt\n",
    "2. Convert the words in the dataset into embeddings using the dictionary from step 1\n",
    "3. Create the embedding layer for keras; this will be the first layer of our DL model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WUHqg2vvb5_l",
    "outputId": "ef13822d-014f-4c4f-d2a1-fc6c774f20ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Found 400000 word vectors in Glove embeddings.\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100 \n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# first, build index mapping words in the embeddings set to their embedding vector\n",
    "#  every line in glove.6B.100d.txt contains the word followed by the embedding vector\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'),encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors in Glove embeddings.' % len(embeddings_index))\n",
    "\n",
    "# prepare embedding matrix - rows are the words from word_index, columns are the embeddings of that word from glove.\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RKasBWVnpFqe",
    "outputId": "396ac2ac-87eb-4021-eca7-dd59706fa869"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing of embedding matrix is done\n"
     ]
    }
   ],
   "source": [
    "# load these pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed during training\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_LEN,\n",
    "                            trainable=False)\n",
    "print(\"Preparing of embedding matrix is done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEastnX8gdxR"
   },
   "source": [
    "# Training and evaluating the DL model\n",
    "We will test 3 DL models:\n",
    "- 1D CNN-based architecture\n",
    "- LSTM-based architecture\n",
    "- Transformer-based architecture (to do it on your own)\n",
    "\n",
    "### 1D CNN Model with pre-trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "bWzIHj5EhLdo"
   },
   "outputs": [],
   "source": [
    "labels_index = {}  # create an empty dictionary\n",
    "for label in train_labels:\n",
    "    if label not in labels_index:\n",
    "        labels_index[label] = len(labels_index)  # add new label to dictionary with next integer index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jdJ2Y2xg2XK",
    "outputId": "de362d6a-47a7-4a2f-fdfb-a5dd692cd8bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define a 1D CNN model.\n",
      "157/157 [==============================] - 14s 18ms/step - loss: 0.6934 - acc: 0.4949 - val_loss: 0.6936 - val_acc: 0.4949\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6934 - acc: 0.5000\n",
      "Test accuracy with CNN: 0.5000200271606445\n"
     ]
    }
   ],
   "source": [
    "print('Define a 1D CNN model.')\n",
    "\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(embedding_layer)\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(GlobalMaxPooling1D())\n",
    "cnnmodel.add(Dense(128, activation='relu'))\n",
    "cnnmodel.add(Dense(len(labels_index), activation='softmax'))\n",
    "\n",
    "cnnmodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "#Train the model. Tune to validation set. \n",
    "cnnmodel.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=1, validation_data=(x_val, y_val))\n",
    "#Evaluate on test set:\n",
    "score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
    "print('Test accuracy with CNN:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdDj2FJzgi_W"
   },
   "source": [
    "### 1D CNN model with training your own embedding\n",
    "The only difference here is that the embedding layer we created ``embedding_layer`` using the pre-trained glove embeddings is no longer used here. We initialize an ambedding layer with randomly initialized weights ``Embedding(MAX_NUM_WORDS, 128)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zI0bISwRb5_w",
    "outputId": "798b8ee4-2c17-43b0-de9d-f61fe9fd577c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\n",
      "157/157 [==============================] - 4s 16ms/step - loss: 0.6933 - acc: 0.5027 - val_loss: 0.6931 - val_acc: 0.5051\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6932 - acc: 0.5005\n",
      "Test accuracy with CNN: 0.5005000233650208\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\")\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(GlobalMaxPooling1D())\n",
    "cnnmodel.add(Dense(128, activation='relu'))\n",
    "cnnmodel.add(Dense(len(labels_index), activation='softmax'))\n",
    "\n",
    "cnnmodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "#Train the model. Tune to validation set. \n",
    "cnnmodel.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=1, validation_data=(x_val, y_val))\n",
    "#Evaluate on test set:\n",
    "score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
    "print('Test accuracy with CNN:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GwhXpmSgt4H"
   },
   "source": [
    "### LSTM Model with training your own embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SvBt2Brib5_4",
    "outputId": "9dffdfae-7a3b-4268-ac5e-aaaa45be2696"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining and training an LSTM model, training embedding layer on the fly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the RNN\n",
      "625/625 [==============================] - 1166s 2s/step - loss: 0.6931 - accuracy: 0.5009 - val_loss: 0.6931 - val_accuracy: 0.5013\n",
      "782/782 [==============================] - 213s 272ms/step - loss: 0.6932 - accuracy: 0.5000\n",
      "Test accuracy with RNN: 0.49998000264167786\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining and training an LSTM model, training embedding layer on the fly\")\n",
    "\n",
    "#model\n",
    "rnnmodel = Sequential()\n",
    "rnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
    "rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "rnnmodel.add(Dense(2, activation='sigmoid'))\n",
    "rnnmodel.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print('Training the RNN')\n",
    "\n",
    "rnnmodel.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=1,\n",
    "          validation_data=(x_val, y_val))\n",
    "score, acc = rnnmodel.evaluate(test_data, test_labels,\n",
    "                            batch_size=32)\n",
    "print('Test accuracy with RNN:', acc)\n",
    "#Test accuracy with RNN: 0.82998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJYzsZFSg9z-"
   },
   "source": [
    "### LSTM Model using pre-trained Embedding Layer\n",
    "\n",
    "> Indented block\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eymx0IyCb5_-",
    "outputId": "b64519cc-64bb-4b06-e77d-587926a10fd3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining and training an LSTM model, using pre-trained embedding layer\n",
      "Training the RNN\n",
      "625/625 [==============================] - 845s 1s/step - loss: 0.6935 - accuracy: 0.4981 - val_loss: 0.6931 - val_accuracy: 0.5013\n",
      "782/782 [==============================] - 213s 273ms/step - loss: 0.6931 - accuracy: 0.5000\n",
      "Test accuracy with RNN: 0.5000200271606445\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining and training an LSTM model, using pre-trained embedding layer\")\n",
    "\n",
    "rnnmodel2 = Sequential()\n",
    "rnnmodel2.add(embedding_layer)\n",
    "rnnmodel2.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "rnnmodel2.add(Dense(2, activation='sigmoid'))\n",
    "rnnmodel2.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print('Training the RNN')\n",
    "\n",
    "rnnmodel2.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=1,\n",
    "          validation_data=(x_val, y_val))\n",
    "score, acc = rnnmodel2.evaluate(test_data, test_labels,\n",
    "                            batch_size=32)\n",
    "print('Test accuracy with RNN:', acc)\n",
    "#Test accuracy with RNN: 0.793"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qW-3VH_joQqj"
   },
   "source": [
    "### Transformer Model \n",
    "Refer to the [keras tutorial](https://keras.io/examples/nlp/text_classification_with_transformer/) to implement and evaluate your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "b3_z0-g9pgzL"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "mPGtUr63pjrV"
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bv-k0OQsonJ3",
    "outputId": "1a746590-cd18-4243-c8ac-12a5ab543994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 Training sequences\n",
      "4999 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200 \n",
    "\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "y966K7bupAP3"
   },
   "outputs": [],
   "source": [
    "#embed_dim = 32  # Embedding size for each token\n",
    "#num_heads = 2  # Number of attention heads\n",
    "#ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "#logits = logits.reshape((64,))\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WiOtNBWNpoPu",
    "outputId": "f3066546-a006-4aaf-cd20-7c281d469e5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "625/625 [==============================] - 9s 8ms/step - loss: 0.6950 - accuracy: 0.4974 - val_loss: 0.6931 - val_accuracy: 0.5051\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.6934 - accuracy: 0.4988 - val_loss: 0.6931 - val_accuracy: 0.5051\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
